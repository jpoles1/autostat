{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import warnings\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "from interpret import set_visualize_provider\n",
    "from interpret.provider import InlineProvider\n",
    "set_visualize_provider(InlineProvider())\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from interpret.glassbox import ExplainableBoostingClassifier, ExplainableBoostingRegressor\n",
    "from interpret import show\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(base_url=\"http://THOTH.local:1234/v1/\", api_key=\"na\", temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen_template = '''\n",
    "Generate an artificial medical research dataset containing 50 columns.\n",
    "The subject of this dataset is: {subject}.\n",
    "\n",
    "Generate a total of 100 rows and return the dataset as a CSV object.\n",
    "'''\n",
    "datagen_prompt = PromptTemplate(\n",
    "    template=datagen_template,\n",
    "    input_variables=[\"subject\"],\n",
    ")\n",
    "datagen_chain = datagen_prompt | llm\n",
    "datagen_output = datagen_chain.invoke({\"subject\": \"age at first stroke\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the CSV data from the blueprint\n",
    "resp = datagen_output.content\n",
    "#extract the part of the string surrounded by ```csv\n",
    "csv_start = resp.find(\"```csv\")\n",
    "csv_end = resp.find(\"```\", csv_start+1)\n",
    "csv_data = resp[csv_start+6:csv_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is a CSV object with 50 columns and 100 rows representing data on age at first stroke:\\n\\n```csv\\nAge,Gender,Race,Ethnicity,Smoker,Alcohol_Consumption,Blood_Pressure,Pulse_Rate,Cholesterol,HDL_Cholesterol,Trial,Medication1,Medication2,Medication3,Fasting_Glucose,Diabetes,Hypertension,Hyperlipidemia,Atrial_Fibrillation,Heart_Disease,Stroke_Type,Stroke_Severity,Time_to_Onset,Mobility,Living_Arrangements,Cognitive_Function,Depression,Sleep_Disorder,Anxiety,Pain,Activity_Level,Body_Mass_Index,Blood_Glucose,Fasting_Bilirubin,Prothrombin_Time,Partial_Thromboplastin_Time,International_Normalized_Ratio,Age_at_Onset_of_First_Stroke\\n25,Male,Caucasian,Non-Hispanic White,No,Light,120/80,70,180,60,Trial1,Painkiller,N/A,N/A,100,0,0,0,0,Ischemic,Severe,2 years,Independent,Living with family,Normal,Asymptomatic,Mild,Insomnia,Low,3.5,95,1.2,11.8,1.4,1\\n45,Female,African American,Non-Hispanic Black,Yes,Heavy,140/90,80,200,50,Trial2,N/A,Acetaminophen,N/A,110,0,1,0,0,Hemorrhagic,Moderate,5 years,Dependent,Living alone,Impaired,Anxious,Severe,Sleep Apnea,High,3.9,105,1.4,12.2,1.6,2\\n68,Male,Asian,Pacific Islander,No,Light,130/85,65,190,70,Trial3,N/A,N/A,N/A,120,0,1,1,0,Ischemic,Mild,7 years,Independent,Living with spouse,Normal,Symptomatic,Moderate,OCD,Low,3.2,90,1.1,11.5,1.3,3\\n...\\n```\\n\\nNote: This is just a sample dataset and the values are randomly generated for demonstration purposes only. In real-world scenarios, you would use actual medical data collected from patients or other sources.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Race</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Smoker</th>\n",
       "      <th>Alcohol_Consumption</th>\n",
       "      <th>Blood_Pressure</th>\n",
       "      <th>Pulse_Rate</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>HDL_Cholesterol</th>\n",
       "      <th>Trial</th>\n",
       "      <th>Medication1</th>\n",
       "      <th>Medication2</th>\n",
       "      <th>Medication3</th>\n",
       "      <th>Fasting_Glucose</th>\n",
       "      <th>Diabetes</th>\n",
       "      <th>Hypertension</th>\n",
       "      <th>Hyperlipidemia</th>\n",
       "      <th>Atrial_Fibrillation</th>\n",
       "      <th>Heart_Disease</th>\n",
       "      <th>Stroke_Type</th>\n",
       "      <th>Stroke_Severity</th>\n",
       "      <th>Time_to_Onset</th>\n",
       "      <th>Mobility</th>\n",
       "      <th>Living_Arrangements</th>\n",
       "      <th>Cognitive_Function</th>\n",
       "      <th>Depression</th>\n",
       "      <th>Sleep_Disorder</th>\n",
       "      <th>Anxiety</th>\n",
       "      <th>Pain</th>\n",
       "      <th>Activity_Level</th>\n",
       "      <th>Body_Mass_Index</th>\n",
       "      <th>Blood_Glucose</th>\n",
       "      <th>Fasting_Bilirubin</th>\n",
       "      <th>Prothrombin_Time</th>\n",
       "      <th>Partial_Thromboplastin_Time</th>\n",
       "      <th>International_Normalized_Ratio</th>\n",
       "      <th>Age_at_Onset_of_First_Stroke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Male</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Non-Hispanic White</td>\n",
       "      <td>No</td>\n",
       "      <td>Light</td>\n",
       "      <td>120/80</td>\n",
       "      <td>70.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>Trial1</td>\n",
       "      <td>Painkiller</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Ischemic</td>\n",
       "      <td>Severe</td>\n",
       "      <td>2 years</td>\n",
       "      <td>Independent</td>\n",
       "      <td>Living with family</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Asymptomatic</td>\n",
       "      <td>Mild</td>\n",
       "      <td>Insomnia</td>\n",
       "      <td>Low</td>\n",
       "      <td>3.5</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>11.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>Female</td>\n",
       "      <td>African American</td>\n",
       "      <td>Non-Hispanic Black</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Heavy</td>\n",
       "      <td>140/90</td>\n",
       "      <td>80.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>Trial2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acetaminophen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Hemorrhagic</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>5 years</td>\n",
       "      <td>Dependent</td>\n",
       "      <td>Living alone</td>\n",
       "      <td>Impaired</td>\n",
       "      <td>Anxious</td>\n",
       "      <td>Severe</td>\n",
       "      <td>Sleep Apnea</td>\n",
       "      <td>High</td>\n",
       "      <td>3.9</td>\n",
       "      <td>105.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>12.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68</td>\n",
       "      <td>Male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Pacific Islander</td>\n",
       "      <td>No</td>\n",
       "      <td>Light</td>\n",
       "      <td>130/85</td>\n",
       "      <td>65.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>Trial3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Ischemic</td>\n",
       "      <td>Mild</td>\n",
       "      <td>7 years</td>\n",
       "      <td>Independent</td>\n",
       "      <td>Living with spouse</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Symptomatic</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>OCD</td>\n",
       "      <td>Low</td>\n",
       "      <td>3.2</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>11.5</td>\n",
       "      <td>1.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Gender              Race           Ethnicity Smoker  \\\n",
       "0   25    Male         Caucasian  Non-Hispanic White     No   \n",
       "1   45  Female  African American  Non-Hispanic Black    Yes   \n",
       "2   68    Male             Asian    Pacific Islander     No   \n",
       "3  ...     NaN               NaN                 NaN    NaN   \n",
       "\n",
       "  Alcohol_Consumption Blood_Pressure  Pulse_Rate  Cholesterol  \\\n",
       "0               Light         120/80        70.0        180.0   \n",
       "1               Heavy         140/90        80.0        200.0   \n",
       "2               Light         130/85        65.0        190.0   \n",
       "3                 NaN            NaN         NaN          NaN   \n",
       "\n",
       "   HDL_Cholesterol   Trial Medication1    Medication2  Medication3  \\\n",
       "0             60.0  Trial1  Painkiller            NaN          NaN   \n",
       "1             50.0  Trial2         NaN  Acetaminophen          NaN   \n",
       "2             70.0  Trial3         NaN            NaN          NaN   \n",
       "3              NaN     NaN         NaN            NaN          NaN   \n",
       "\n",
       "   Fasting_Glucose  Diabetes  Hypertension  Hyperlipidemia  \\\n",
       "0            100.0       0.0           0.0             0.0   \n",
       "1            110.0       0.0           1.0             0.0   \n",
       "2            120.0       0.0           1.0             1.0   \n",
       "3              NaN       NaN           NaN             NaN   \n",
       "\n",
       "   Atrial_Fibrillation Heart_Disease Stroke_Type Stroke_Severity  \\\n",
       "0                  0.0      Ischemic      Severe         2 years   \n",
       "1                  0.0   Hemorrhagic    Moderate         5 years   \n",
       "2                  0.0      Ischemic        Mild         7 years   \n",
       "3                  NaN           NaN         NaN             NaN   \n",
       "\n",
       "  Time_to_Onset            Mobility Living_Arrangements Cognitive_Function  \\\n",
       "0   Independent  Living with family              Normal       Asymptomatic   \n",
       "1     Dependent        Living alone            Impaired            Anxious   \n",
       "2   Independent  Living with spouse              Normal        Symptomatic   \n",
       "3           NaN                 NaN                 NaN                NaN   \n",
       "\n",
       "  Depression Sleep_Disorder Anxiety  Pain  Activity_Level  Body_Mass_Index  \\\n",
       "0       Mild       Insomnia     Low   3.5            95.0              1.2   \n",
       "1     Severe    Sleep Apnea    High   3.9           105.0              1.4   \n",
       "2   Moderate            OCD     Low   3.2            90.0              1.1   \n",
       "3        NaN            NaN     NaN   NaN             NaN              NaN   \n",
       "\n",
       "   Blood_Glucose  Fasting_Bilirubin  Prothrombin_Time  \\\n",
       "0           11.8                1.4               1.0   \n",
       "1           12.2                1.6               2.0   \n",
       "2           11.5                1.3               3.0   \n",
       "3            NaN                NaN               NaN   \n",
       "\n",
       "   Partial_Thromboplastin_Time  International_Normalized_Ratio  \\\n",
       "0                          NaN                             NaN   \n",
       "1                          NaN                             NaN   \n",
       "2                          NaN                             NaN   \n",
       "3                          NaN                             NaN   \n",
       "\n",
       "   Age_at_Onset_of_First_Stroke  \n",
       "0                           NaN  \n",
       "1                           NaN  \n",
       "2                           NaN  \n",
       "3                           NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#parse the CSV data into a pandas dataframe\n",
    "raw_data = pd.read_csv(StringIO(csv_data))\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "def best_cont_dist(col):\n",
    "    col = col.dropna()  # Remove any missing values\n",
    "    col = col.astype(float)  # Convert to float\n",
    "    # Calculate basic statistics\n",
    "    mean = np.mean(col)\n",
    "    median = np.median(col)\n",
    "    std_dev = np.std(col)\n",
    "    \n",
    "    # Fit different distributions and find the best fit\n",
    "    distributions = [\n",
    "        stats.uniform,  # Uniform distribution\n",
    "        stats.norm,  # Normal distribution\n",
    "        stats.expon,  # Exponential distribution\n",
    "        stats.gamma,  # Gamma distribution\n",
    "        stats.lognorm,  # Log-normal distribution\n",
    "        stats.beta,  # Beta distribution\n",
    "    ]\n",
    "    \n",
    "    best_fit = None\n",
    "    best_fit_name = \"\"\n",
    "    best_fit_params = ()\n",
    "    best_fit_error = np.inf\n",
    "    \n",
    "    for distribution in distributions:\n",
    "        try:\n",
    "            # Fit the distribution to the data\n",
    "            params = distribution.fit(col)\n",
    "            \n",
    "            # Calculate the error between the fitted distribution and the data\n",
    "            error = stats.kstest(col, distribution.name, args=params).statistic\n",
    "            \n",
    "            # Update the best fit if the error is lower\n",
    "            if error < best_fit_error:\n",
    "                best_fit = distribution\n",
    "                best_fit_name = distribution.name\n",
    "                best_fit_params = params\n",
    "                best_fit_error = error\n",
    "        except Exception as e:\n",
    "            print(f\"Error fitting {distribution.name}: {e}\")\n",
    "    \n",
    "    return {\n",
    "        \"best_fit_distribution\": best_fit_name,\n",
    "        \"best_fit_params\": best_fit_params,\n",
    "        \"best_fit_error\": best_fit_error\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fitting uniform: zero-size array to reduction operation minimum which has no identity\n",
      "Error fitting norm: attempt to get argmax of an empty sequence\n",
      "Error fitting expon: zero-size array to reduction operation minimum which has no identity\n",
      "Error fitting gamma: zero-size array to reduction operation minimum which has no identity\n",
      "Error fitting lognorm: zero-size array to reduction operation minimum which has no identity\n",
      "Error fitting beta: zero-size array to reduction operation minimum which has no identity\n",
      "Error fitting gamma: Optimization converged to parameters that are outside the range allowed by the distribution.\n",
      "Error fitting gamma: Optimization converged to parameters that are outside the range allowed by the distribution.\n",
      "Error fitting uniform: zero-size array to reduction operation minimum which has no identity\n",
      "Error fitting norm: attempt to get argmax of an empty sequence\n",
      "Error fitting expon: zero-size array to reduction operation minimum which has no identity\n",
      "Error fitting gamma: zero-size array to reduction operation minimum which has no identity\n",
      "Error fitting lognorm: zero-size array to reduction operation minimum which has no identity\n",
      "Error fitting beta: zero-size array to reduction operation minimum which has no identity\n",
      "Error fitting uniform: zero-size array to reduction operation minimum which has no identity\n",
      "Error fitting norm: attempt to get argmax of an empty sequence\n",
      "Error fitting expon: zero-size array to reduction operation minimum which has no identity\n",
      "Error fitting gamma: zero-size array to reduction operation minimum which has no identity\n",
      "Error fitting lognorm: zero-size array to reduction operation minimum which has no identity\n",
      "Error fitting beta: zero-size array to reduction operation minimum which has no identity\n",
      "Error fitting uniform: zero-size array to reduction operation minimum which has no identity\n",
      "Error fitting norm: attempt to get argmax of an empty sequence\n",
      "Error fitting expon: zero-size array to reduction operation minimum which has no identity\n",
      "Error fitting gamma: zero-size array to reduction operation minimum which has no identity\n",
      "Error fitting lognorm: zero-size array to reduction operation minimum which has no identity\n",
      "Error fitting beta: zero-size array to reduction operation minimum which has no identity\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'Age',\n",
       "  'type': 'object',\n",
       "  'summary': {'25': 1, '45': 1, '68': 1, '...': 1},\n",
       "  'dist': None},\n",
       " {'name': 'Gender',\n",
       "  'type': 'object',\n",
       "  'summary': {'Male': 2, 'Female': 1},\n",
       "  'dist': None},\n",
       " {'name': 'Race',\n",
       "  'type': 'object',\n",
       "  'summary': {'Caucasian': 1, 'African American': 1, 'Asian': 1},\n",
       "  'dist': None},\n",
       " {'name': 'Ethnicity',\n",
       "  'type': 'object',\n",
       "  'summary': {'Non-Hispanic White': 1,\n",
       "   'Non-Hispanic Black': 1,\n",
       "   'Pacific Islander': 1},\n",
       "  'dist': None},\n",
       " {'name': 'Smoker',\n",
       "  'type': 'object',\n",
       "  'summary': {'No': 2, 'Yes': 1},\n",
       "  'dist': None},\n",
       " {'name': 'Alcohol_Consumption',\n",
       "  'type': 'object',\n",
       "  'summary': {'Light': 2, 'Heavy': 1},\n",
       "  'dist': None},\n",
       " {'name': 'Blood_Pressure',\n",
       "  'type': 'object',\n",
       "  'summary': {'120/80': 1, '140/90': 1, '130/85': 1},\n",
       "  'dist': None},\n",
       " {'name': 'Pulse_Rate',\n",
       "  'type': 'float64',\n",
       "  'summary': {'count': 3.0,\n",
       "   'mean': 71.66666666666667,\n",
       "   'std': 7.637626158259734,\n",
       "   'min': 65.0,\n",
       "   '25%': 67.5,\n",
       "   '50%': 70.0,\n",
       "   '75%': 75.0,\n",
       "   'max': 80.0},\n",
       "  'dist': {'best_fit_distribution': 'norm',\n",
       "   'best_fit_params': (71.66666666666667, 6.236095644623236),\n",
       "   'best_fit_error': 0.27203265359952633}},\n",
       " {'name': 'Cholesterol',\n",
       "  'type': 'float64',\n",
       "  'summary': {'count': 3.0,\n",
       "   'mean': 190.0,\n",
       "   'std': 10.0,\n",
       "   'min': 180.0,\n",
       "   '25%': 185.0,\n",
       "   '50%': 190.0,\n",
       "   '75%': 195.0,\n",
       "   'max': 200.0},\n",
       "  'dist': {'best_fit_distribution': 'norm',\n",
       "   'best_fit_params': (190.0, 8.16496580927726),\n",
       "   'best_fit_error': 0.22299765237340996}},\n",
       " {'name': 'HDL_Cholesterol',\n",
       "  'type': 'float64',\n",
       "  'summary': {'count': 3.0,\n",
       "   'mean': 60.0,\n",
       "   'std': 10.0,\n",
       "   'min': 50.0,\n",
       "   '25%': 55.0,\n",
       "   '50%': 60.0,\n",
       "   '75%': 65.0,\n",
       "   'max': 70.0},\n",
       "  'dist': {'best_fit_distribution': 'norm',\n",
       "   'best_fit_params': (60.0, 8.16496580927726),\n",
       "   'best_fit_error': 0.22299765237340996}},\n",
       " {'name': 'Trial',\n",
       "  'type': 'object',\n",
       "  'summary': {'Trial1': 1, 'Trial2': 1, 'Trial3': 1},\n",
       "  'dist': None},\n",
       " {'name': 'Medication1',\n",
       "  'type': 'object',\n",
       "  'summary': {'Painkiller': 1},\n",
       "  'dist': None},\n",
       " {'name': 'Medication2',\n",
       "  'type': 'object',\n",
       "  'summary': {'Acetaminophen': 1},\n",
       "  'dist': None},\n",
       " {'name': 'Medication3',\n",
       "  'type': 'float64',\n",
       "  'summary': {'count': 0.0,\n",
       "   'mean': nan,\n",
       "   'std': nan,\n",
       "   'min': nan,\n",
       "   '25%': nan,\n",
       "   '50%': nan,\n",
       "   '75%': nan,\n",
       "   'max': nan},\n",
       "  'dist': {'best_fit_distribution': '',\n",
       "   'best_fit_params': (),\n",
       "   'best_fit_error': inf}},\n",
       " {'name': 'Fasting_Glucose',\n",
       "  'type': 'float64',\n",
       "  'summary': {'count': 3.0,\n",
       "   'mean': 110.0,\n",
       "   'std': 10.0,\n",
       "   'min': 100.0,\n",
       "   '25%': 105.0,\n",
       "   '50%': 110.0,\n",
       "   '75%': 115.0,\n",
       "   'max': 120.0},\n",
       "  'dist': {'best_fit_distribution': 'norm',\n",
       "   'best_fit_params': (110.0, 8.16496580927726),\n",
       "   'best_fit_error': 0.22299765237340996}},\n",
       " {'name': 'Diabetes',\n",
       "  'type': 'float64',\n",
       "  'summary': {'count': 3.0,\n",
       "   'mean': 0.0,\n",
       "   'std': 0.0,\n",
       "   'min': 0.0,\n",
       "   '25%': 0.0,\n",
       "   '50%': 0.0,\n",
       "   '75%': 0.0,\n",
       "   'max': 0.0},\n",
       "  'dist': {'best_fit_distribution': 'beta',\n",
       "   'best_fit_params': (0.9899920243471434,\n",
       "    1.0393951944685846,\n",
       "    -2.661587125184533e-26,\n",
       "    9.609767701594333e-26),\n",
       "   'best_fit_error': 0.7102180791478286}},\n",
       " {'name': 'Hypertension',\n",
       "  'type': 'float64',\n",
       "  'summary': {'count': 3.0,\n",
       "   'mean': 0.6666666666666666,\n",
       "   'std': 0.5773502691896258,\n",
       "   'min': 0.0,\n",
       "   '25%': 0.5,\n",
       "   '50%': 1.0,\n",
       "   '75%': 1.0,\n",
       "   'max': 1.0},\n",
       "  'dist': {'best_fit_distribution': 'gamma',\n",
       "   'best_fit_params': (227.13567923326457,\n",
       "    -6.60549167558489,\n",
       "    0.03201538105457831),\n",
       "   'best_fit_error': 0.42571478846818206}},\n",
       " {'name': 'Hyperlipidemia',\n",
       "  'type': 'float64',\n",
       "  'summary': {'count': 3.0,\n",
       "   'mean': 0.3333333333333333,\n",
       "   'std': 0.5773502691896258,\n",
       "   'min': 0.0,\n",
       "   '25%': 0.0,\n",
       "   '50%': 0.0,\n",
       "   '75%': 0.5,\n",
       "   'max': 1.0},\n",
       "  'dist': {'best_fit_distribution': 'norm',\n",
       "   'best_fit_params': (0.3333333333333333, 0.4714045207910317),\n",
       "   'best_fit_error': 0.4269166055731899}},\n",
       " {'name': 'Atrial_Fibrillation',\n",
       "  'type': 'float64',\n",
       "  'summary': {'count': 3.0,\n",
       "   'mean': 0.0,\n",
       "   'std': 0.0,\n",
       "   'min': 0.0,\n",
       "   '25%': 0.0,\n",
       "   '50%': 0.0,\n",
       "   '75%': 0.0,\n",
       "   'max': 0.0},\n",
       "  'dist': {'best_fit_distribution': 'beta',\n",
       "   'best_fit_params': (0.9899920243471434,\n",
       "    1.0393951944685846,\n",
       "    -2.661587125184533e-26,\n",
       "    9.609767701594333e-26),\n",
       "   'best_fit_error': 0.7102180791478286}},\n",
       " {'name': 'Heart_Disease',\n",
       "  'type': 'object',\n",
       "  'summary': {'Ischemic': 2, 'Hemorrhagic': 1},\n",
       "  'dist': None},\n",
       " {'name': 'Stroke_Type',\n",
       "  'type': 'object',\n",
       "  'summary': {'Severe': 1, 'Moderate': 1, 'Mild': 1},\n",
       "  'dist': None},\n",
       " {'name': 'Stroke_Severity',\n",
       "  'type': 'object',\n",
       "  'summary': {'2 years': 1, '5 years': 1, '7 years': 1},\n",
       "  'dist': None},\n",
       " {'name': 'Time_to_Onset',\n",
       "  'type': 'object',\n",
       "  'summary': {'Independent': 2, 'Dependent': 1},\n",
       "  'dist': None},\n",
       " {'name': 'Mobility',\n",
       "  'type': 'object',\n",
       "  'summary': {'Living with family': 1,\n",
       "   'Living alone': 1,\n",
       "   'Living with spouse': 1},\n",
       "  'dist': None},\n",
       " {'name': 'Living_Arrangements',\n",
       "  'type': 'object',\n",
       "  'summary': {'Normal': 2, 'Impaired': 1},\n",
       "  'dist': None},\n",
       " {'name': 'Cognitive_Function',\n",
       "  'type': 'object',\n",
       "  'summary': {'Asymptomatic': 1, 'Anxious': 1, 'Symptomatic': 1},\n",
       "  'dist': None},\n",
       " {'name': 'Depression',\n",
       "  'type': 'object',\n",
       "  'summary': {'Mild': 1, 'Severe': 1, 'Moderate': 1},\n",
       "  'dist': None},\n",
       " {'name': 'Sleep_Disorder',\n",
       "  'type': 'object',\n",
       "  'summary': {'Insomnia': 1, 'Sleep Apnea': 1, 'OCD': 1},\n",
       "  'dist': None},\n",
       " {'name': 'Anxiety',\n",
       "  'type': 'object',\n",
       "  'summary': {'Low': 2, 'High': 1},\n",
       "  'dist': None},\n",
       " {'name': 'Pain',\n",
       "  'type': 'float64',\n",
       "  'summary': {'count': 3.0,\n",
       "   'mean': 3.5333333333333337,\n",
       "   'std': 0.3511884584284245,\n",
       "   'min': 3.2,\n",
       "   '25%': 3.35,\n",
       "   '50%': 3.5,\n",
       "   '75%': 3.7,\n",
       "   'max': 3.9},\n",
       "  'dist': {'best_fit_distribution': 'gamma',\n",
       "   'best_fit_params': (9.81865911454991,\n",
       "    2.645429441430485,\n",
       "    0.0900078110262525),\n",
       "   'best_fit_error': 0.23115765171184066}},\n",
       " {'name': 'Activity_Level',\n",
       "  'type': 'float64',\n",
       "  'summary': {'count': 3.0,\n",
       "   'mean': 96.66666666666667,\n",
       "   'std': 7.637626158259734,\n",
       "   'min': 90.0,\n",
       "   '25%': 92.5,\n",
       "   '50%': 95.0,\n",
       "   '75%': 100.0,\n",
       "   'max': 105.0},\n",
       "  'dist': {'best_fit_distribution': 'norm',\n",
       "   'best_fit_params': (96.66666666666667, 6.236095644623236),\n",
       "   'best_fit_error': 0.27203265359952633}},\n",
       " {'name': 'Body_Mass_Index',\n",
       "  'type': 'float64',\n",
       "  'summary': {'count': 3.0,\n",
       "   'mean': 1.2333333333333332,\n",
       "   'std': 0.15275252316519458,\n",
       "   'min': 1.1,\n",
       "   '25%': 1.15,\n",
       "   '50%': 1.2,\n",
       "   '75%': 1.2999999999999998,\n",
       "   'max': 1.4},\n",
       "  'dist': {'best_fit_distribution': 'norm',\n",
       "   'best_fit_params': (1.2333333333333332, 0.12472191289246465),\n",
       "   'best_fit_error': 0.2720326535995257}},\n",
       " {'name': 'Blood_Glucose',\n",
       "  'type': 'float64',\n",
       "  'summary': {'count': 3.0,\n",
       "   'mean': 11.833333333333334,\n",
       "   'std': 0.3511884584284242,\n",
       "   'min': 11.5,\n",
       "   '25%': 11.65,\n",
       "   '50%': 11.8,\n",
       "   '75%': 12.0,\n",
       "   'max': 12.2},\n",
       "  'dist': {'best_fit_distribution': 'gamma',\n",
       "   'best_fit_params': (6.852792722656687,\n",
       "    11.073103124347078,\n",
       "    0.11014141429267754),\n",
       "   'best_fit_error': 0.22722348219927857}},\n",
       " {'name': 'Fasting_Bilirubin',\n",
       "  'type': 'float64',\n",
       "  'summary': {'count': 3.0,\n",
       "   'mean': 1.4333333333333333,\n",
       "   'std': 0.1527525231651947,\n",
       "   'min': 1.3,\n",
       "   '25%': 1.35,\n",
       "   '50%': 1.4,\n",
       "   '75%': 1.5,\n",
       "   'max': 1.6},\n",
       "  'dist': {'best_fit_distribution': 'norm',\n",
       "   'best_fit_params': (1.4333333333333333, 0.12472191289246475),\n",
       "   'best_fit_error': 0.27203265359952633}},\n",
       " {'name': 'Prothrombin_Time',\n",
       "  'type': 'float64',\n",
       "  'summary': {'count': 3.0,\n",
       "   'mean': 2.0,\n",
       "   'std': 1.0,\n",
       "   'min': 1.0,\n",
       "   '25%': 1.5,\n",
       "   '50%': 2.0,\n",
       "   '75%': 2.5,\n",
       "   'max': 3.0},\n",
       "  'dist': {'best_fit_distribution': 'norm',\n",
       "   'best_fit_params': (2.0, 0.816496580927726),\n",
       "   'best_fit_error': 0.22299765237340996}},\n",
       " {'name': 'Partial_Thromboplastin_Time',\n",
       "  'type': 'float64',\n",
       "  'summary': {'count': 0.0,\n",
       "   'mean': nan,\n",
       "   'std': nan,\n",
       "   'min': nan,\n",
       "   '25%': nan,\n",
       "   '50%': nan,\n",
       "   '75%': nan,\n",
       "   'max': nan},\n",
       "  'dist': {'best_fit_distribution': '',\n",
       "   'best_fit_params': (),\n",
       "   'best_fit_error': inf}},\n",
       " {'name': 'International_Normalized_Ratio',\n",
       "  'type': 'float64',\n",
       "  'summary': {'count': 0.0,\n",
       "   'mean': nan,\n",
       "   'std': nan,\n",
       "   'min': nan,\n",
       "   '25%': nan,\n",
       "   '50%': nan,\n",
       "   '75%': nan,\n",
       "   'max': nan},\n",
       "  'dist': {'best_fit_distribution': '',\n",
       "   'best_fit_params': (),\n",
       "   'best_fit_error': inf}},\n",
       " {'name': 'Age_at_Onset_of_First_Stroke',\n",
       "  'type': 'float64',\n",
       "  'summary': {'count': 0.0,\n",
       "   'mean': nan,\n",
       "   'std': nan,\n",
       "   'min': nan,\n",
       "   '25%': nan,\n",
       "   '50%': nan,\n",
       "   '75%': nan,\n",
       "   'max': nan},\n",
       "  'dist': {'best_fit_distribution': '',\n",
       "   'best_fit_params': (),\n",
       "   'best_fit_error': inf}}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a function which summarizes every column in a pandas dataframe (both numeric and categorical) and returns a json array which can then be used by a large language model to select relevant columns for answering a specific question\n",
    "def summarize_dataframe(df):\n",
    "    summary = []\n",
    "    \n",
    "    # Iterate over each column in the dataframe\n",
    "    for column in df.columns:\n",
    "        column_summary = {}\n",
    "        column_summary['name'] = column\n",
    "        column_summary['type'] = str(df[column].dtype)\n",
    "\n",
    "        # Check if the column is boolean\n",
    "        if df[column].dtype == 'bool':\n",
    "            column_summary['summary'] = df[column].value_counts().to_dict()\n",
    "            column_summary[\"dist\"] = \"Binary\"                \n",
    "\n",
    "        # Check if the column is numeric\n",
    "        elif pd.api.types.is_numeric_dtype(df[column]):\n",
    "            column_summary['summary'] = df[column].describe().to_dict()\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter('ignore')\n",
    "                column_summary[\"dist\"] = best_cont_dist(df[column])\n",
    "                        \n",
    "        else:\n",
    "            column_summary['summary'] = df[column].value_counts().to_dict()\n",
    "            column_summary[\"dist\"] = None\n",
    "        \n",
    "        summary.append(column_summary)\n",
    "    \n",
    "    return summary\n",
    "summary = summarize_dataframe(raw_data)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGuruSchema(BaseModel):\n",
    "    predictor_cols: List[str] = Field(description=\"Dataset columns which will be used to predict the outcome variables.\")\n",
    "    output_cols: List[str] = Field(description=\"Dataset columns which will be predicted by the input columns.\")\n",
    "    excluded_cols: List[str] = Field(description=\"Dataset columns which will not be used in the model.\")\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = JsonOutputParser(pydantic_object=DataGuruSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser_to_prompt_schema(parser):\n",
    "    schema = {k: v for k, v in parser._get_schema(parser.pydantic_object).items()}\n",
    "\n",
    "    # Remove extraneous fields.\n",
    "    reduced_schema = schema\n",
    "    if \"title\" in reduced_schema:\n",
    "        del reduced_schema[\"title\"]\n",
    "    if \"type\" in reduced_schema:\n",
    "        del reduced_schema[\"type\"]\n",
    "    # Ensure json in context is well-formed with double quotes.\n",
    "    schema_str = json.dumps(reduced_schema)\n",
    "\n",
    "    return schema_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchQuestionSchema(BaseModel):\n",
    "    question: str = Field(description=\"The research question to be answered by the model.\")\n",
    "    outcome: str = Field(description=\"A single outcome column from the dataset to be predicted by the model.\")\n",
    "    predictors: List[str] = Field(description=\"The input columns to be used to predict the outcome column. Try to include as many columns as possible to improve the model's accuracy.\")\n",
    "    model_type: List[str] = Field(description=\"The sklearn model types to be used for prediction. May include multiple to find the best option. Options include: 'LinearRegression', 'LogisticRegression', 'RandomForestClassifier', 'RandomForestRegressor', 'GradientBoostingClassifier', 'GradientBoostingRegressor', 'SVC', 'KNeighborsClassifier', 'KNeighborsRegressor', 'DecisionTreeClassifier', 'DecisionTreeRegressor', etc\")\n",
    "class ResearcherSchema(BaseModel):\n",
    "    topics: List[ResearchQuestionSchema] = Field(description=\"A list of research questions to be answered by the model.\")\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "researcher_parser = JsonOutputParser(pydantic_object=ResearcherSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Age',\n",
       "  'type': 'object',\n",
       "  'summary': {'25': 1, '45': 1, '68': 1, '...': 1},\n",
       "  'dist': None},\n",
       " {'name': 'Gender',\n",
       "  'type': 'object',\n",
       "  'summary': {'Male': 2, 'Female': 1},\n",
       "  'dist': None},\n",
       " {'name': 'Race',\n",
       "  'type': 'object',\n",
       "  'summary': {'Caucasian': 1, 'African American': 1, 'Asian': 1},\n",
       "  'dist': None},\n",
       " {'name': 'Ethnicity',\n",
       "  'type': 'object',\n",
       "  'summary': {'Non-Hispanic White': 1,\n",
       "   'Non-Hispanic Black': 1,\n",
       "   'Pacific Islander': 1},\n",
       "  'dist': None},\n",
       " {'name': 'Smoker',\n",
       "  'type': 'object',\n",
       "  'summary': {'No': 2, 'Yes': 1},\n",
       "  'dist': None},\n",
       " {'name': 'Alcohol_Consumption',\n",
       "  'type': 'object',\n",
       "  'summary': {'Light': 2, 'Heavy': 1},\n",
       "  'dist': None},\n",
       " {'name': 'Blood_Pressure',\n",
       "  'type': 'object',\n",
       "  'summary': {'120/80': 1, '140/90': 1, '130/85': 1},\n",
       "  'dist': None},\n",
       " {'name': 'Pulse_Rate',\n",
       "  'type': 'float64',\n",
       "  'summary': {'count': 3.0,\n",
       "   'mean': 71.66666666666667,\n",
       "   'std': 7.637626158259734,\n",
       "   'min': 65.0,\n",
       "   '25%': 67.5,\n",
       "   '50%': 70.0,\n",
       "   '75%': 75.0,\n",
       "   'max': 80.0},\n",
       "  'dist': {'best_fit_distribution': 'norm',\n",
       "   'best_fit_params': (71.66666666666667, 6.236095644623236),\n",
       "   'best_fit_error': 0.27203265359952633}},\n",
       " {'name': 'Cholesterol',\n",
       "  'type': 'float64',\n",
       "  'summary': {'count': 3.0,\n",
       "   'mean': 190.0,\n",
       "   'std': 10.0,\n",
       "   'min': 180.0,\n",
       "   '25%': 185.0,\n",
       "   '50%': 190.0,\n",
       "   '75%': 195.0,\n",
       "   'max': 200.0},\n",
       "  'dist': {'best_fit_distribution': 'norm',\n",
       "   'best_fit_params': (190.0, 8.16496580927726),\n",
       "   'best_fit_error': 0.22299765237340996}},\n",
       " {'name': 'HDL_Cholesterol',\n",
       "  'type': 'float64',\n",
       "  'summary': {'count': 3.0,\n",
       "   'mean': 60.0,\n",
       "   'std': 10.0,\n",
       "   'min': 50.0,\n",
       "   '25%': 55.0,\n",
       "   '50%': 60.0,\n",
       "   '75%': 65.0,\n",
       "   'max': 70.0},\n",
       "  'dist': {'best_fit_distribution': 'norm',\n",
       "   'best_fit_params': (60.0, 8.16496580927726),\n",
       "   'best_fit_error': 0.22299765237340996}},\n",
       " {'name': 'Trial',\n",
       "  'type': 'object',\n",
       "  'summary': {'Trial1': 1, 'Trial2': 1, 'Trial3': 1},\n",
       "  'dist': None},\n",
       " {'name': 'Medication1',\n",
       "  'type': 'object',\n",
       "  'summary': {'Painkiller': 1},\n",
       "  'dist': None},\n",
       " {'name': 'Medication2',\n",
       "  'type': 'object',\n",
       "  'summary': {'Acetaminophen': 1},\n",
       "  'dist': None},\n",
       " {'name': 'Medication3',\n",
       "  'type': 'float64',\n",
       "  'summary': {'count': 0.0,\n",
       "   'mean': nan,\n",
       "   'std': nan,\n",
       "   'min': nan,\n",
       "   '25%': nan,\n",
       "   '50%': nan,\n",
       "   '75%': nan,\n",
       "   'max': nan},\n",
       "  'dist': {'best_fit_distribution': '',\n",
       "   'best_fit_params': (),\n",
       "   'best_fit_error': inf}},\n",
       " {'name': 'Fasting_Glucose',\n",
       "  'type': 'float64',\n",
       "  'summary': {'count': 3.0,\n",
       "   'mean': 110.0,\n",
       "   'std': 10.0,\n",
       "   'min': 100.0,\n",
       "   '25%': 105.0,\n",
       "   '50%': 110.0,\n",
       "   '75%': 115.0,\n",
       "   'max': 120.0},\n",
       "  'dist': {'best_fit_distribution': 'norm',\n",
       "   'best_fit_params': (110.0, 8.16496580927726),\n",
       "   'best_fit_error': 0.22299765237340996}},\n",
       " {'name': 'Diabetes',\n",
       "  'type': 'float64',\n",
       "  'summary': {'count': 3.0,\n",
       "   'mean': 0.0,\n",
       "   'std': 0.0,\n",
       "   'min': 0.0,\n",
       "   '25%': 0.0,\n",
       "   '50%': 0.0,\n",
       "   '75%': 0.0,\n",
       "   'max': 0.0},\n",
       "  'dist': {'best_fit_distribution': 'beta',\n",
       "   'best_fit_params': (0.9899920243471434,\n",
       "    1.0393951944685846,\n",
       "    -2.661587125184533e-26,\n",
       "    9.609767701594333e-26),\n",
       "   'best_fit_error': 0.7102180791478286}},\n",
       " {'name': 'Hypertension',\n",
       "  'type': 'float64',\n",
       "  'summary': {'count': 3.0,\n",
       "   'mean': 0.6666666666666666,\n",
       "   'std': 0.5773502691896258,\n",
       "   'min': 0.0,\n",
       "   '25%': 0.5,\n",
       "   '50%': 1.0,\n",
       "   '75%': 1.0,\n",
       "   'max': 1.0},\n",
       "  'dist': {'best_fit_distribution': 'gamma',\n",
       "   'best_fit_params': (227.13567923326457,\n",
       "    -6.60549167558489,\n",
       "    0.03201538105457831),\n",
       "   'best_fit_error': 0.42571478846818206}},\n",
       " {'name': 'Hyperlipidemia',\n",
       "  'type': 'float64',\n",
       "  'summary': {'count': 3.0,\n",
       "   'mean': 0.3333333333333333,\n",
       "   'std': 0.5773502691896258,\n",
       "   'min': 0.0,\n",
       "   '25%': 0.0,\n",
       "   '50%': 0.0,\n",
       "   '75%': 0.5,\n",
       "   'max': 1.0},\n",
       "  'dist': {'best_fit_distribution': 'norm',\n",
       "   'best_fit_params': (0.3333333333333333, 0.4714045207910317),\n",
       "   'best_fit_error': 0.4269166055731899}},\n",
       " {'name': 'Atrial_Fibrillation',\n",
       "  'type': 'float64',\n",
       "  'summary': {'count': 3.0,\n",
       "   'mean': 0.0,\n",
       "   'std': 0.0,\n",
       "   'min': 0.0,\n",
       "   '25%': 0.0,\n",
       "   '50%': 0.0,\n",
       "   '75%': 0.0,\n",
       "   'max': 0.0},\n",
       "  'dist': {'best_fit_distribution': 'beta',\n",
       "   'best_fit_params': (0.9899920243471434,\n",
       "    1.0393951944685846,\n",
       "    -2.661587125184533e-26,\n",
       "    9.609767701594333e-26),\n",
       "   'best_fit_error': 0.7102180791478286}},\n",
       " {'name': 'Heart_Disease',\n",
       "  'type': 'object',\n",
       "  'summary': {'Ischemic': 2, 'Hemorrhagic': 1},\n",
       "  'dist': None},\n",
       " {'name': 'Stroke_Type',\n",
       "  'type': 'object',\n",
       "  'summary': {'Severe': 1, 'Moderate': 1, 'Mild': 1},\n",
       "  'dist': None},\n",
       " {'name': 'Stroke_Severity',\n",
       "  'type': 'object',\n",
       "  'summary': {'2 years': 1, '5 years': 1, '7 years': 1},\n",
       "  'dist': None},\n",
       " {'name': 'Time_to_Onset',\n",
       "  'type': 'object',\n",
       "  'summary': {'Independent': 2, 'Dependent': 1},\n",
       "  'dist': None},\n",
       " {'name': 'Mobility',\n",
       "  'type': 'object',\n",
       "  'summary': {'Living with family': 1,\n",
       "   'Living alone': 1,\n",
       "   'Living with spouse': 1},\n",
       "  'dist': None},\n",
       " {'name': 'Living_Arrangements',\n",
       "  'type': 'object',\n",
       "  'summary': {'Normal': 2, 'Impaired': 1},\n",
       "  'dist': None},\n",
       " {'name': 'Cognitive_Function',\n",
       "  'type': 'object',\n",
       "  'summary': {'Asymptomatic': 1, 'Anxious': 1, 'Symptomatic': 1},\n",
       "  'dist': None},\n",
       " {'name': 'Depression',\n",
       "  'type': 'object',\n",
       "  'summary': {'Mild': 1, 'Severe': 1, 'Moderate': 1},\n",
       "  'dist': None},\n",
       " {'name': 'Sleep_Disorder',\n",
       "  'type': 'object',\n",
       "  'summary': {'Insomnia': 1, 'Sleep Apnea': 1, 'OCD': 1},\n",
       "  'dist': None},\n",
       " {'name': 'Anxiety',\n",
       "  'type': 'object',\n",
       "  'summary': {'Low': 2, 'High': 1},\n",
       "  'dist': None},\n",
       " {'name': 'Pain',\n",
       "  'type': 'float64',\n",
       "  'summary': {'count': 3.0,\n",
       "   'mean': 3.5333333333333337,\n",
       "   'std': 0.3511884584284245,\n",
       "   'min': 3.2,\n",
       "   '25%': 3.35,\n",
       "   '50%': 3.5,\n",
       "   '75%': 3.7,\n",
       "   'max': 3.9},\n",
       "  'dist': {'best_fit_distribution': 'gamma',\n",
       "   'best_fit_params': (9.81865911454991,\n",
       "    2.645429441430485,\n",
       "    0.0900078110262525),\n",
       "   'best_fit_error': 0.23115765171184066}},\n",
       " {'name': 'Activity_Level',\n",
       "  'type': 'float64',\n",
       "  'summary': {'count': 3.0,\n",
       "   'mean': 96.66666666666667,\n",
       "   'std': 7.637626158259734,\n",
       "   'min': 90.0,\n",
       "   '25%': 92.5,\n",
       "   '50%': 95.0,\n",
       "   '75%': 100.0,\n",
       "   'max': 105.0},\n",
       "  'dist': {'best_fit_distribution': 'norm',\n",
       "   'best_fit_params': (96.66666666666667, 6.236095644623236),\n",
       "   'best_fit_error': 0.27203265359952633}},\n",
       " {'name': 'Body_Mass_Index',\n",
       "  'type': 'float64',\n",
       "  'summary': {'count': 3.0,\n",
       "   'mean': 1.2333333333333332,\n",
       "   'std': 0.15275252316519458,\n",
       "   'min': 1.1,\n",
       "   '25%': 1.15,\n",
       "   '50%': 1.2,\n",
       "   '75%': 1.2999999999999998,\n",
       "   'max': 1.4},\n",
       "  'dist': {'best_fit_distribution': 'norm',\n",
       "   'best_fit_params': (1.2333333333333332, 0.12472191289246465),\n",
       "   'best_fit_error': 0.2720326535995257}},\n",
       " {'name': 'Blood_Glucose',\n",
       "  'type': 'float64',\n",
       "  'summary': {'count': 3.0,\n",
       "   'mean': 11.833333333333334,\n",
       "   'std': 0.3511884584284242,\n",
       "   'min': 11.5,\n",
       "   '25%': 11.65,\n",
       "   '50%': 11.8,\n",
       "   '75%': 12.0,\n",
       "   'max': 12.2},\n",
       "  'dist': {'best_fit_distribution': 'gamma',\n",
       "   'best_fit_params': (6.852792722656687,\n",
       "    11.073103124347078,\n",
       "    0.11014141429267754),\n",
       "   'best_fit_error': 0.22722348219927857}},\n",
       " {'name': 'Fasting_Bilirubin',\n",
       "  'type': 'float64',\n",
       "  'summary': {'count': 3.0,\n",
       "   'mean': 1.4333333333333333,\n",
       "   'std': 0.1527525231651947,\n",
       "   'min': 1.3,\n",
       "   '25%': 1.35,\n",
       "   '50%': 1.4,\n",
       "   '75%': 1.5,\n",
       "   'max': 1.6},\n",
       "  'dist': {'best_fit_distribution': 'norm',\n",
       "   'best_fit_params': (1.4333333333333333, 0.12472191289246475),\n",
       "   'best_fit_error': 0.27203265359952633}},\n",
       " {'name': 'Prothrombin_Time',\n",
       "  'type': 'float64',\n",
       "  'summary': {'count': 3.0,\n",
       "   'mean': 2.0,\n",
       "   'std': 1.0,\n",
       "   'min': 1.0,\n",
       "   '25%': 1.5,\n",
       "   '50%': 2.0,\n",
       "   '75%': 2.5,\n",
       "   'max': 3.0},\n",
       "  'dist': {'best_fit_distribution': 'norm',\n",
       "   'best_fit_params': (2.0, 0.816496580927726),\n",
       "   'best_fit_error': 0.22299765237340996}},\n",
       " {'name': 'Partial_Thromboplastin_Time',\n",
       "  'type': 'float64',\n",
       "  'summary': {'count': 0.0,\n",
       "   'mean': nan,\n",
       "   'std': nan,\n",
       "   'min': nan,\n",
       "   '25%': nan,\n",
       "   '50%': nan,\n",
       "   '75%': nan,\n",
       "   'max': nan},\n",
       "  'dist': {'best_fit_distribution': '',\n",
       "   'best_fit_params': (),\n",
       "   'best_fit_error': inf}},\n",
       " {'name': 'International_Normalized_Ratio',\n",
       "  'type': 'float64',\n",
       "  'summary': {'count': 0.0,\n",
       "   'mean': nan,\n",
       "   'std': nan,\n",
       "   'min': nan,\n",
       "   '25%': nan,\n",
       "   '50%': nan,\n",
       "   '75%': nan,\n",
       "   'max': nan},\n",
       "  'dist': {'best_fit_distribution': '',\n",
       "   'best_fit_params': (),\n",
       "   'best_fit_error': inf}},\n",
       " {'name': 'Age_at_Onset_of_First_Stroke',\n",
       "  'type': 'float64',\n",
       "  'summary': {'count': 0.0,\n",
       "   'mean': nan,\n",
       "   'std': nan,\n",
       "   'min': nan,\n",
       "   '25%': nan,\n",
       "   '50%': nan,\n",
       "   '75%': nan,\n",
       "   'max': nan},\n",
       "  'dist': {'best_fit_distribution': '',\n",
       "   'best_fit_params': (),\n",
       "   'best_fit_error': inf}}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topics': [{'question': 'What are the factors that contribute to high blood pressure among the study participants?',\n",
       "   'outcome': 'Blood_Pressure',\n",
       "   'predictors': ['Age',\n",
       "    'Gender',\n",
       "    'Race',\n",
       "    'Ethnicity',\n",
       "    'Smoker',\n",
       "    'Alcohol_Consumption'],\n",
       "   'model_type': ['LinearRegression']},\n",
       "  {'question': 'Is there a correlation between cholesterol levels and the risk of heart disease?',\n",
       "   'outcome': 'Heart_Disease',\n",
       "   'predictors': ['Cholesterol',\n",
       "    'HDL_Cholesterol',\n",
       "    'Age',\n",
       "    'Gender',\n",
       "    'Race',\n",
       "    'Ethnicity'],\n",
       "   'model_type': ['LogisticRegression']},\n",
       "  {'question': 'Can we predict the likelihood of developing diabetes based on certain health factors?',\n",
       "   'outcome': 'Diabetes',\n",
       "   'predictors': ['Fasting_Glucose', 'Age', 'Gender', 'Race', 'Ethnicity'],\n",
       "   'model_type': ['RandomForestClassifier']}]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "researcher_template = '''\n",
    "You are a helpful assistant that answers in JSON.\n",
    "\n",
    "Your task is to review a summary of a dataset and develop a list of 3-5 research questions to be answered using the available data.\n",
    "\n",
    "Below is a summary of the dataset in JSON format:\n",
    "{summary}\n",
    "\n",
    "Here's the json schema you must adhere to for your response:\n",
    "<schema>\n",
    "{schema}\n",
    "</schema>\n",
    "'''\n",
    "\n",
    "researcher_prompt = PromptTemplate(\n",
    "    template=researcher_template,\n",
    "    input_variables=[\"subject\", \"summary\"],\n",
    "    partial_variables={\"schema\": parser_to_prompt_schema(researcher_parser)},\n",
    ")\n",
    "researcher_chain = researcher_prompt | llm | parser\n",
    "researcher_output = researcher_chain.invoke({\"summary\": json.dumps({\"columns\": summary})})\n",
    "display(researcher_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research Question: What are the factors that contribute to high blood pressure among the study participants?\n",
      "Outcome: Blood_Pressure\n",
      "Predictors: ['Age', 'Gender', 'Race', 'Ethnicity', 'Smoker', 'Alcohol_Consumption']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jpoles1/.cache/pypoetry/virtualenvs/autostat-MBtPn9pI-py3.12/lib/python3.12/site-packages/interpret/glassbox/_ebm/_utils.py:381: UserWarning: Too few samples per class, adapting test size to guarantee 1 sample per class.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.0\n",
      "Research Question: Is there a correlation between cholesterol levels and the risk of heart disease?\n",
      "Outcome: Heart_Disease\n",
      "Predictors: ['Cholesterol', 'HDL_Cholesterol', 'Age', 'Gender', 'Race', 'Ethnicity']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jpoles1/.cache/pypoetry/virtualenvs/autostat-MBtPn9pI-py3.12/lib/python3.12/site-packages/interpret/glassbox/_ebm/_utils.py:381: UserWarning: Too few samples per class, adapting test size to guarantee 1 sample per class.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.0\n",
      "Research Question: Can we predict the likelihood of developing diabetes based on certain health factors?\n",
      "Outcome: Diabetes\n",
      "Predictors: ['Fasting_Glucose', 'Age', 'Gender', 'Race', 'Ethnicity']\n",
      "Generating a predictor for a continuous value\n",
      "Score: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jpoles1/.cache/pypoetry/virtualenvs/autostat-MBtPn9pI-py3.12/lib/python3.12/site-packages/sklearn/metrics/_regression.py:1211: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "reasearch_results = []\n",
    "\n",
    "for topic in researcher_output[\"topics\"]:\n",
    "    print(f\"Research Question: {topic['question']}\")\n",
    "    print(f\"Outcome: {topic['outcome']}\")\n",
    "    print(f\"Predictors: {topic['predictors']}\")\n",
    "\n",
    "    clean_data = raw_data.dropna(subset=topic[\"predictors\"] + [topic[\"outcome\"]])\n",
    "    \n",
    "    X = clean_data[topic[\"predictors\"]]\n",
    "    y = clean_data[topic[\"outcome\"]]\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    if pd.api.types.is_numeric_dtype(y):\n",
    "        print(\"Generating a predictor for a continuous value\")\n",
    "        ebm = ExplainableBoostingRegressor()\n",
    "        ebm.fit(X_train, y_train)\n",
    "        print(\"Score:\", ebm.score(X_test, y_test))\n",
    "        reasearch_results.append({\"topic\": topic, \"model\": ebm})\n",
    "    else:\n",
    "        ebm = ExplainableBoostingClassifier()\n",
    "        ebm.fit(X_train, y_train)\n",
    "        print(\"Score:\", ebm.score(X_test, y_test))\n",
    "        reasearch_results.append({\"topic\": topic, \"model\": ebm})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictor_cols': ['Age',\n",
       "  'Gender',\n",
       "  'Race',\n",
       "  'Ethnicity',\n",
       "  'Smoker',\n",
       "  'Alcohol_Consumption',\n",
       "  'Blood_Pressure',\n",
       "  'Pulse_Rate',\n",
       "  'Cholesterol',\n",
       "  'HDL_Cholesterol',\n",
       "  'Trial',\n",
       "  'Medication1',\n",
       "  'Medication2',\n",
       "  'Fasting_Glucose',\n",
       "  'Diabetes',\n",
       "  'Hypertension',\n",
       "  'Hyperlipidemia',\n",
       "  'Atrial_Fibrillation',\n",
       "  'Heart_Disease',\n",
       "  'Stroke_Type',\n",
       "  'Stroke_Severity',\n",
       "  'Time_to_Onset',\n",
       "  'Mobility',\n",
       "  'Living_Arrangements',\n",
       "  'Cognitive_Function',\n",
       "  'Depression',\n",
       "  'Sleep_Disorder',\n",
       "  'Anxiety',\n",
       "  'Pain',\n",
       "  'Activity_Level',\n",
       "  'Body_Mass_Index',\n",
       "  'Blood_Glucose',\n",
       "  'Fasting_Bilirubin',\n",
       "  'Prothrombin_Time'],\n",
       " 'output_cols': ['Age_at_Onset_of_First_Stroke'],\n",
       " 'excluded_cols': []}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataguru_template = '''\n",
    "You are a helpful assistant that answers in JSON.\n",
    "\n",
    "Your task is to review a summary of a dataset and return which columns should be included as predictor and outcome variables in a machine learning model analysis.\n",
    "Columns that are irrelevant should be excluded.\n",
    "There may be more than one outcome column.\n",
    "All columns must be categorized into one of the three categories: predictor, outcome, or excluded. Each column must be assigned to only one of these categories.\n",
    "\n",
    "The subject of your investigation is: {subject}.\n",
    "\n",
    "Below is a summary of the dataset in JSON format:\n",
    "{summary}\n",
    "\n",
    "Here's the json schema you must adhere to for your response:\n",
    "<schema>\n",
    "{schema}\n",
    "</schema>\n",
    "'''\n",
    "\n",
    "dataguru_prompt = PromptTemplate(\n",
    "    template=dataguru_template,\n",
    "    input_variables=[\"subject\", \"summary\"],\n",
    "    partial_variables={\"schema\": parser_to_prompt_schema(parser)},\n",
    ")\n",
    "\n",
    "dataguru_chain = dataguru_prompt | llm | parser\n",
    "dataguru_ouptut = dataguru_chain.invoke({\"subject\": \"age at first stroke\", \"summary\": json.dumps({\"columns\": summary})})\n",
    "dataguru_ouptut\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Age_at_Onset_of_First_Stroke'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/autostat-MBtPn9pI-py3.12/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Age_at_Onset_of_First_Stroke'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m clean_data \u001b[38;5;241m=\u001b[39m raw_data\u001b[38;5;241m.\u001b[39mdropna(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[1;32m      2\u001b[0m X \u001b[38;5;241m=\u001b[39m clean_data[dataguru_ouptut[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictor_cols\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m----> 3\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mclean_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataguru_ouptut\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_cols\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/autostat-MBtPn9pI-py3.12/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/autostat-MBtPn9pI-py3.12/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Age_at_Onset_of_First_Stroke'"
     ]
    }
   ],
   "source": [
    "clean_data = raw_data.dropna(axis=1, how=\"all\").dropna()\n",
    "X = clean_data[dataguru_ouptut[\"predictor_cols\"]]\n",
    "y = clean_data[dataguru_ouptut[\"output_cols\"][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mis_numeric_dtype(y):\n\u001b[1;32m      4\u001b[0m     ebm \u001b[38;5;241m=\u001b[39m ExplainableBoostingRegressor()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/autostat-MBtPn9pI-py3.12/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/autostat-MBtPn9pI-py3.12/lib/python3.12/site-packages/sklearn/model_selection/_split.py:2778\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2775\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[1;32m   2777\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m-> 2778\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\n\u001b[1;32m   2780\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2782\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m   2783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/autostat-MBtPn9pI-py3.12/lib/python3.12/site-packages/sklearn/model_selection/_split.py:2408\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2405\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[1;32m   2407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2408\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2409\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2410\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2411\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[1;32m   2412\u001b[0m     )\n\u001b[1;32m   2414\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n",
    "\n",
    "if pd.api.types.is_numeric_dtype(y):\n",
    "    ebm = ExplainableBoostingRegressor()\n",
    "    ebm.fit(X_train, y_train)\n",
    "else:\n",
    "    ebm = ExplainableBoostingClassifier()\n",
    "    ebm.fit(X_train, y_train)\n",
    "    print(\"Score:\", ebm.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebm.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(ebm.explain_global())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autostat-MBtPn9pI-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
